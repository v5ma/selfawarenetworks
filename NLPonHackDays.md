Transcript of Hack Days Podcast on Neural Lace at Noisebridge with Micah Blumberg
December 2017

Youtube link: https://www.youtube.com/watch?v=uNJ6Q72rHG0 
Article link: https://www.svgn.io/p/hack-days-in-san-francisco-c677d753c93f

Well, no, I had a Hackintosh.
A Hackintosh, yeah.
No, I have one of those. It's just really old and doesn't work anymore.
Yeah, that's how it works.
As it usually goes, yeah.
We're live streaming.
What?
We're online. Yeah, it's a camera.
Are you Hack Days?
Yeah, we're Hack Days. Are you here to join Hack Days?
Uh, yeah.
Yeah, come on.
I'm actually in camera.
You can see. What's that?
Yeah, I'm going to sit here.
Okay.
Yes.
Welcome.
Thank you so much.
So, yeah, I'm curious because I'm actually learning Python,
but do you do some projects together?
So, Hack Days was, it started out of us, like, programming at a friend's house on his living room floor,
and then it got too big for that space because we had, like, eight people show up once.
That was crazy.
That was crazy.
We started, which, I mean, it's a small place, right?
It was crazy.
So, we started live streaming it, and then we moved here, and it became more of a live stream
talking about kind of what we're working on, like a show and tell, as well as, like, everything that's happening across, like,
machine learning, bots, cryptocurrency, things like that.
And really, the content depends on who shows up.
So, you know, you can talk about your project or whatever.
I would be excited if something came along that did, like, unsupervised learning that understood and saw the system
and, yeah, ended up being better than anyone else.
Like, that would be super cool.
Also, I want it to be implanted in my brain.
Right.
Like, as a, like, an accessory that you can become a part of it.
Yeah.
Would you take orders from a voice in...
Oh, I guess you might already take orders.
From a voice inside your head.
I'm like, oh, that's me.
I guess we already do.
Divine Intervention.
Would you take orders from a voice inside your head?
Like, if you're plugged into a massive global supercomputer.
What if there is some sort of way, I mean, I guess this is getting more sci-fi a little bit, but...
Is it though?
Well, I guess it's getting more science.
No.
Just without the fiction.
Yeah.
But, like, isn't it also possible that you could have it, like, change chemicals in your head to not only tell you...
Not even tell you, like, it wouldn't just be a voice in your head.
It would just be...
Right.
Like a sense?
You mean?
It would be like your subconscious mind telling you to do it.
Right.
As opposed to your conscious mind hearing a voice.
It would become your subconscious...
Uh...
So you would think it's coming from yourself.
Yeah.
Exactly.
It would be...
Or you wouldn't even think about it coming from yourself.
You would just...
You would know that it's coming from yourself.
Yeah.
Like, you would think that you know, like...
You would be so influenced by this voice.
That's terrifying.
You would have the desire.
It would become a desire.
It's will would be your desire.
Then who are you?
Ooh...
Who are you?
I'm nothing.
I'm a pile of trash.
The way that I kind of think about it is almost like, um...
Those, uh...
Like those ants.
With, like, the fungal...
Yeah.
The fungi in their brain.
Full of it.
Where it's like, they don't know why they're doing it, but they know that's what they're supposed to be doing.
Or, like, the snails?
Yeah, the snails that have, like, the...
They have, like, that, um...
Like, that kind of, like, weird, like, parasite in them.
Yeah, yeah, yeah.
That goes into their eyes, so they get eaten by a bird.
Yeah, they want to climb up, uh...
A tree.
A tree.
To get eaten by a bird.
Yeah.
Because it's part of that thing's life cycle.
What?
Is to get eaten by a bird.
And then to get pooped out by the bird.
And then that's when it lays, like, the babies and all that stuff, so...
Yeah.
Yeah.
Hey.
Hey, how's it going?
If you want to join us...
So, we're streaming live now, if you want to be on the show.
Wow.
Wow.
So, where's it inside the field of the camera?
Uh, right here.
Yeah.
So, you could sit here and be in the...
You'll be in the thing.
...in the shot.
But you would have to come around here.
Let me come around there.
Okay.
Okay.
Yeah.
It's like a jungle gym over there.
It is.
Sorry.
Yeah.
Hey, I don't want to be, like, in the front of the camera.
It depends if I...
Yeah.
Yeah.
Hi.
Hi.
Micah.
Yeah.
Pleasure meeting you.
Yeah, I actually did, um, a podcast with this guy.
Yeah.
A neural lace podcast.
Yeah.
And we lost the data.
Oh.
We did.
We walked around the city and we talked about the neural lace.
Yeah.
Right?
Yeah.
Yeah.
We did.
What is a neural lace?
Neural lace is next-generation brainless mirror interface.
Yeah.
So we were talking about how that might work.
And...
Anyway.
Yeah.
So we're, uh...
So Hack Days basically talks about anything you want.
So what are you working on lately?
Um, so right now, um, I am, uh, so I'm organizing SFBR events.
San Francisco Rich Reality.
Cool.
Um, and I'm also, um, I have Silicon Valley Global News.
So I'm going to conventions and getting news, collecting news stories.
And filming them on Facebook Live.
And then, later on, adding them to my blog.
Yes.
You just know that I am.
Nice.
Good.
Yeah.
And then I still do, like, um, like, I started doing, like, what I call the Nerve Deer Show.
Yeah.
Just like, again, just like renaming their own neural lace.
But the idea was, um, I did it in Facebook Spaces with, uh, with, um...
Oh, Facebook Spaces is a VR, like, where everyone looks like a cartoon kind of thing?
Yeah, yeah.
Okay.
That is so sick.
Really?
From, from Spaces?
From VR.
Wow.
That's awesome.
Yeah.
And so that was called the Nerve Deer Show.
I did it with a neuroscientist who comes around here.
I don't know if her name is Sarah, but it's just Shotsky or something.
But, um, and then I did, uh, a follow-up.
So that didn't get huge ratings.
I did a follow-up and, and we called the show Death Star World.
Because a lot of the, a lot of the technologies involved with, um, researching neural lace could be
happening with a lot of creating, um, a technological singularity that's self-aware, a self-aware network.
Right.
So, um, I have a group on Facebook called Self-aware Networks, and all we do is we research, you
know, um, you know, how the brain might work and how we can hack into it.
Sweet.
Yeah.
Yeah.
Yeah.
That's cool.
And having great brains and artificial cortex.
Wow.
Yeah.
I mean, it's, I think it's a cool topic, so, to rally people around.
And so what's great is, like, I have a neurophysicist group that has 9,000 people in it.
And I'm like, yes, 9,000 people are interested in, uh, diving deep into the physics of neurons.
Yeah.
And approaching it from that perspective.
It, it seems like a very niche topic, and 9,000 people on a very niche topic about what
is literally the future.
Yeah.
It seems really exciting.
Yeah.
Do you get a lot of people who are very academic, or a lot of people who are kind of armchair,
or, like, a lot of people who are just interested in, like, what that future will facilitate?
Like, what is the audience like there?
Um, well, without knowing each of those 9,000 people.
Hmm.
You don't know each of those?
I don't know each of them individually, but, like, I suspect that there's a good amount
of armchair, but every once in a while we get, like, a really legit, um, neuroscientist
who joins one of our, one of our groups.
Yeah.
Oh, wow.
In some cases, we'll, on rare occasion, we'll get, like, a famous person who joined
one of these, like, self-aware networks.
That's cool.
Yeah.
It's funny because, um, have you seen what we've been working on?
Uh, I've been watching Actives.
With Synapse?
Okay.
I don't know a lot about it.
Yeah, you should check out Synapse.
Okay.
We're, uh, doing decentralized AI.
Right.
Yeah.
Oh, yeah.
Okay.
So, um...
I'm, I'm, I'm, I'm, I'm, yeah, I, I think there's a lot of, like, parallels every time we
meet up.
Yeah.
So, tell me about Synapse.
Yeah.
Uh, I don't want to pitch it now.
Okay.
I mean, we could, we could pitch it later.
I'm, I'm more curious to bring in, because I, I literally pitch it every time.
You pitch it every time.
Yeah.
And so people are, like, like, the same as our radio.
Right, right.
So, tell us more about, you know, what's happening with you now, um, in terms of, like,
your research, and then where, where you're headed in the future.
Yeah.
What's next?
What's next?
Um, well, okay, so what I want to do next is, um, I want to start coming down to Noise
Bridge, which is where you, you are on Sundays.
But the reason is because there is a, uh, I guess there's a neural hacking space here.
Yeah.
And what I want to do, so I guess they have an OpenBCI headset, right?
Yeah.
Um, so I'm actually with, uh, developer, I'm part of a group called the Vision Agency,
and they make a product called the Microdose VR.
The what, what agency?
The Vision Agency.
Vision Agency, okay.
Yeah, led by Andrew Jones, and they make Microdose VR, right?
It's really, it's like one of the most pretty.
What is Microdose VR?
So Microdose VR is this Vive application that, um, you know, you put on your Vive,
HTC Vive headset, and you have your two controllers, and you are able to paint the most, um, amazing,
uh, 3D, um, polygons into space.
And they, but what's, what's great is that they, we also have like an audio rack there.
So we have a DJ who works on the team.
Mm-hmm.
Um, his name is Blue, his, his handle is Blue Tech, right?
So when you paint these, these, uh, polygons in space, and they're so, like,
there's almost four of those, it's like it's actually there.
Mm-hmm.
It's really, really neat.
But then they start pulsing with the music of the DJ.
And so you're like creating the music with your hands.
Yeah.
Oh, wow.
That's awesome.
Yeah.
And then we got this, and then, um, so Android is friends with,
Andrew Jones is friends with the CEO of Vias.
And so they got this face plate, which has EEG sensors in it, and heart rate sensor.
So we're like taking the heart rate sensor and making your heart rate cost changes to the,
to the size and shape and vibration of all the polygons that you're making.
Oh, nice.
In addition to them being auto-graphic.
So your biofeedback is actually changing the graphics around.
You know, we, uh, this week went out to CCA.
We were asked to judge, uh, some industrial design students' projects.
Mm-hmm.
And a lot of them revolved around, uh, biofeedback.
And a lot of them, um, there was one that had, um, and I, I think we have video of this
that we'll be putting up as well.
One that had, um, lights, uh, that sense, uh, it, it connected to an EEG headset.
So you had basically like home appliances that would interact and react to what you are
and what you're doing.
And, um, one of the things that I thought was interesting is like, okay, not only can
they reflect, but they can provide feedback and set the way that they can help you converge
on some particular state of being, right?
Yeah.
Um, I thought that was pretty fascinating.
Like that was a big theme there.
Um, another one, um, was this really interesting, and in, in some ways it's kind of a feedback
group, um, where they would basically turn babies in their surveillance machines, uh,
so that it would record all the smells, all the sights, all the sounds of being you as
a baby, right?
Yeah, you can run deep on neural networks on that data and try to pretend.
Yeah, but I don't think they realize what they're doing.
Sure, but they didn't realize that.
But I thought it would be cool as if, um, you could replay that as you were expiring,
and like, you could be a baby again as you're leaving the world as like a, this 2000 Space
Odyssey thing, right?
Yeah.
Yeah.
Right?
They were kind of like, yeah, totally.
Because what they also did was they turned the experience into a virtual reality experience,
into like a, well, not virtual reality, but you could replay it, right?
Yeah.
From the baby's perspective.
Yeah.
And it was really like, when I put it on, I was like, I'm a baby now.
Yeah.
Because everyone was giant in that thing, you know?
Like, yeah, but it's so interesting that we're starting to see that thematically
more and more, right?
Of like...
People, there's a convergence.
Right.
Of science and technology.
Yeah.
Um, people want to mess around with effective sensors and, you know, typically stuff in
the domain of neuroscience.
They want to bring it into VR and they want to bring it, you know, blockchain and artificial
intelligence and all these different buzzwords.
Yeah.
They want to bring the technologies behind those buzzwords together.
Right.
And people are doing it.
Kind of like what I focus on.
It's like about local natives.
Oh, cool.
We should talk, chat then.
Yeah.
Because, you know, I was, uh, preaching AI and blockchain when everyone was like, that's
crazy.
Yeah.
You know?
There was actually the focus of my last event at San Francisco Virtual Reality was, uh,
I had a speaker, uh, from Decentraline.
He was talking about web VR and blockchain.
Yeah.
And I had a speaker from, um, gosh, I'm forgetting their name.
Oh, map.
Mapping Aggregation Platform.
Um, and they, they were combining, um, blockchain with location-based augmented reality.
Location-based services with augmented reality.
So what that means is that, um, uh, that retail location.
Wait.
Did they work out of here?
I don't know.
Because I feel like I, I know that person.
That's awesome.
That's awesome.
Go ahead.
Tell her.
Pitch us this idea.
Great.
The location is on the blockchain.
Yeah.
And so when you go into their location, um, you're served up, um, you know, your AR headset
has served up the content that they want, they would, they would like you to consider
looking at while you're visiting their location.
Yeah.
That would be amazing.
Yeah.
That just makes sense.
Yeah.
So it's a really great idea.
They're, they're, they're, so they're combining, like this is, I think this idea, um, I've
discussed it with people, different people, but now we're talking about bringing blockchain
into location-based services.
Mm-hmm.
And, and, you know, bringing location-based services to AR was something people have already
talked about before.
Yeah.
It's like, you know, the coolest stuff keeps on getting cooler by combining more real
stuff.
I agree.
And people are like, oh, that's a lot of buzzwords in one sentence.
I'm like, but do you understand the implications?
Yeah.
Like the emergence of a product out of these things, like I'm sure capacitors were a buzzword
at one point, you know?
Yeah.
A transistor door, right?
Yeah.
So, so right now, um, Intel is asking, um, microdose VR, or actually asking Android,
who's, who leads the vision agency team to, to create a version of microdose for the
neural headset, which is like an EG headset that's designed to fit the HSC vibe.
Mm-hmm.
Um, and that's great.
I mean, they're like a competitor to me, as we're already working with.
Cool.
But what I want to do here, the reason I want to maybe start a project here, is like,
what if we could take the Open DCI headset and create a version of it, like,
a 3D printed version of it here, with the 3D printers in this building.
Yeah.
And so that it fits the HSC vibe.
So, and the ArcGIS script.
Right.
Right.
So that everybody can 3D print their own headset for their vibe.
I think it's last mile stuff like that, that, uh, companies like Apple and Google
and Vibe, like, they should be striving for.
And they probably want, right?
Right.
If they, if you can get the last mile feedback of how people are actually engaging with what's
in front of them.
Like, truly.
Not just, like, eyes.
Not just expression.
But, like, mentally engaging.
Like, I mean, that's, that's awesome.
That's, like, next level stuff, right?
Yeah.
Yeah.
Well, that's awesome.
Uh, so we're, we're throwing the Decentralized AI conference, actually.
And we would love...
Oh, you're organizing that?
Yeah, yeah, yeah.
Uh, well, we just announced it.
Right?
And so we would like to, uh, maybe figure out a way we'd partner, because it sounds like
you're, you're definitely in the same space or something.
Yeah.
Yeah.
Organize events.
Sounds just weird.
Reality.
But I, I like to participate in the community as much as possible.
Yeah.
Um, I do demo of microdose VR.
Cool.
So, you know, if you guys want to do demo there.
Yeah, for sure.
That would be sick.
That would be amazing.
Yeah.
Yes.
High five.
That would be so sick.
Yeah.
Nice.
It would be wireless.
Woo!
It's great.
I love, love the demo, because every time someone gets in there, they just like, they,
first of all, it takes a few minutes for them to like figure out, and then they just
start dancing.
Right.
Yeah.
How cool.
That's great.
That's super awesome.
So what, what else is happening in, in your world that you're excited about?
That other, other projects that are happening, or other advancements, like?
Um, well, uh...
Cause you're like on the bleeding edge.
You know what's happening, right?
Yeah.
So I know that there's a company in San Diego, um, that is bringing sensors to the
market.
They're like a billion times improvement over EEG sensors.
Really?
You can figure them to measure like a single, like, problem class.
Really?
Like that.
Um, and so that's another reason why I'm like pushing the open, let's do OpenBCI so
we can modify it so it works with VR, so we can, so we can take their product.
Yeah.
I'm just like very forward thinking in that respect.
Yeah.
Um, so, what, okay, so maybe we can like, you know, so one thing that I talked about
on the podcast with you is actually there are, um, there are people out there in this world
who are like a joint twin, and their brains are like physically connected.
And they're connected, and there's, there's, um, you know, I don't wanna like, you know,
that you can, people can like, you know, search for stories with joint twins.
I'm not gonna name anything specifically.
Right.
Right.
Um, but.
Like when you mean connected, you mean like can fill one another's lofts and things like
that.
Like their skulls are, in their brains are physically grown together.
Oh, physically.
Okay.
Yeah.
I thought you meant.
Yeah.
Um, because their skulls are physically connected, like one twin can, they can't see through,
um, they can't see the same thing at the same time.
Because, if you know, if, if, if one girl, if one person wants to turn your head to the
left, the other person's head moves farther to the left.
They can't look at the same television at the same time.
But you can ask, you can ask the twin who's not looking at the television, uh, what their
twin is seeing.
And, and they can, and.
Oh, that's weird.
The first twin can tell you.
So they're able to see through the other twin's eyes.
Wow.
And it's interesting because of where their brains are connected.
They're connected, um, partially through the thalamus and, and like, and above that, like
the thalamus.
And it's like, it makes me think, well, if you can see through, they can see through each
other's eyes and they can taste what each other tastes.
Um, one of the twins loves ketchup.
Um, one of them hates ketchup.
Oh, no.
So they can torture each other.
Like, the twin that loves ketchup and, and the other twin's like, ew, gross, ew.
You know.
Oh my gosh.
Right.
How old are they?
So, um, I think they're like, I think they're adults, yeah.
Oh.
Yeah.
The idea is that, you know, if, if, um, if the thalamus or certain parts of the brain
that they're, they're connected by, allows you to, uh, send and receive sensory information
between the two brains.
Mm-hmm.
Um, you know, visual information and taste information.
And maybe that part of the brain could be the target for a neural lace, which is what
we talked about.
Mm-hmm.
This podcast that we lost, unfortunately, because I lost the part of it.
But.
And it was great.
The best podcast you've ever heard.
Yeah.
Yeah.
And that one is just like, really hard to get to.
Yeah.
So, um, I've been like, you know, you know, you know, talking to companies that have really
exotic sensors, like the one in San Diego that I mentioned, um, to see if.
So when you talk about neural lace, are you talking about implants?
Both.
I'm talking about, you know, because we want, because we want to explore, you know, for
research purposes, um, implants for people who just have extreme, you know, medical situations
and stuff.
Um, but we also want, there's so many, the thing is we have so many great wireless technologies
on this planet.
Right.
So it, it just has to be very, it's very plausible to me that, um, that we should be able to find
a wireless technology that can, um, uh, both, both, uh, uh, send and receive what's in the
deep part of the brain.
Yeah, that would be amazing.
Um, and that may not be the, it may not even be the best part of the brain to the target
for neural lace.
I mean, it could be that because the brain is a very general learning algorithm.
You could eventually, like, attach to any part of the brain and send and receive information.
Um, that's a hypothesis.
But, you know, like, so David Eagleman talks about, um, you know, how you can plug in, like,
you can, you can take a, you can take a rat's eyes and plug it into the part of the brain
that's the rat's ears.
Mm-hmm.
And the brain, the rat's brain will figure it out.
And it will, the eyes will start functioning normally.
Wow.
It's like the cortex itself is a very general learning, uh, program.
So it can be reprogrammed.
Yeah.
You could, you could put in 10 new eyeballs.
You could put stock data.
It's one of David's ideas.
You could plug stock data into your brain and eventually your brain will figure it out.
It could be pretty stock better than anyone else.
Interesting.
Yeah.
Interesting.
Like bypassing all hardware neural networks.
Right.
Right.
So if someone's blind, what they're doing is they have a tongue strip.
Yeah.
Right.
Or back strip.
And so the cameras, signals, uh, uh, on the camera sensor are turning into electrodes
that simulate the tongue.
And the brain figures out visual data from that.
And so people can see, people who are completely blind can see through a camera because of
that, um, sort of exterior bioport.
Wow.
So it's possible that, you know, I think that, you know, we could target like maybe the
thalamus and put a sensor up the nose surgically.
Yeah.
Um, but it might be possible that we could just, um.
Sign me up.
Yeah, same.
I'll reverse, I'll reverse total recall a sensor.
Yeah.
Yeah.
But it might be that we could do it from any part of the brain and it might be that we
could do it from, like, so if you could plug into any part of the brain, then the question
is, well, how do you, like, if the brain is like a network, how do you browse that network?
Yeah.
What's the web browser?
What's the web browsing protocol?
Yeah.
Um, is it more like, is, is, is.
And how can I erase my browsing history?
Yeah.
There you go.
Yeah.
Is it more like TCP?
Is it more like UDP?
Right.
What is the communication protocol?
Probably UDP, I think.
You think?
Yeah.
Well, I mean, I get different from different folks.
Really?
The reason I always say TCP is, like, imagine in the double-slit experiment when you're observing
something and you're actually affecting it.
So it's like, it's not just like the packet of data is thrown at your eye, the photons are
thrown at your eye.
It's like, you get, it's like a handshake because you get the data and then something is sent
back that affects what's physically there in the double-slit of the observer experiment.
Yeah.
That would be like a handshake.
There's a feedback loop between you and what you're observing.
But if we're just talking about, like...
A TCP is like a feedback loop.
It's like a handshake.
Right.
Interesting.
Yeah.
I would still say, like, some, there's, it's lossy.
It would have to be lossy.
Unless, like, the loss is a parameter of its output.
So we have unconscious information.
We have conscious information.
Right.
And what if, like, the key difference, um, one of the key differences there could be that
when it's conscious, it's in a feedback loop.
It's data and it's, it has temporarily active data.
Mm-hmm.
And it's like, but when it's unconscious, it's not in a feedback loop.
So then it's more like UDP.
Yeah.
And it's like a packet that's just being programmed.
Anyway.
No, I agree.
Sorry.
I'm, uh, gonna be posting what TCP is versus UDP to our audience so they know.
Oh, yeah, yeah.
They're not all hardcore hackers, I think.
There might be one or two that aren't.
Yeah.
So there's some great, um, you know, neuroscientists that I follow.
Like Olaf Swans, who talks about networks of the brain.
Mm-hmm.
Um, there was this, um, this book that I read in 2005.
How do you spell the name?
Olaf.
Um, O-L-A-F, and then last name is S-P-O-R-N-S.
Okay.
There was a book you read?
Yeah, that, uh, he wrote in the book, Networks of the Brain.
Um, and then there's this, there's a book called The Neural Basis of Free Will Criterial
Pausation, which is, uh, something I really recommend.
Because, uh, they talk about how neurons become, neurons as going to detectors.
They can detect if there are two signals split within three milliseconds.
Or if it's a variable amount of milliseconds.
But, um, that can, so the neurons can, if it detects that coincidence, it can fire.
Mm-hmm.
And that's one, uh, one way for a neuron to detect coincidences.
I mean, there's, there's a lot going on with neurons, you know, dendroids.
And dendroids can, or some people are saying they're less mini computers.
Mm-hmm.
They have their own, you know, ability to store complications over a long time.
And then trigger themselves to fire them depending on some circumstances.
Mm-hmm.
Um, so anyway, so there's, like, a lot of people, um, this book.
Uh, of course, a lot of people have read, you know, Douglas Hoffs and others on the Strangely.
But I think that kind of gives the, the idea that we are, in a sense, um, a kind of strange feedback loop.
Yeah.
And the human brain is, like, this, um, um, is like a hierarchy of feedback loops.
Mm-hmm.
Um, small feedback loops and large feedback loops.
Like, so your, your sensory information comes in through your eyes and your ears.
Yeah.
Like, if, you know, the photons bounce off your eyes, and you can, and, um, and they cause the proteins and the ganglia now to click.
And it causes the separation of positive and negative charges.
And, you know, the calcium and potassium ions.
And then eventually, you know, just like a lightning strike, there's a, there's an action potential, right?
Because there's, there's, it has to re-normalize.
It has to rebalance itself.
Mm-hmm.
And so you have this surge of electrical signals along the optic nerve to the thalamus.
And so the eyes and ears, and every sense except for the nose, goes through the thalamus first.
So it's like that sensory equation is in the thalamus first.
And then it goes from the thalamus back into, like, so the eyes go back into the, the occipital lobes and the frital lobes,
and then through the rest of the brain, um, you know, your visual cortex back here.
But then it all comes back around to the thalamus, which is like the top of the pyramid of the neocortex.
So it's like a big figure back from your eyes to the thalamus, to the neocortex, back around through the neocortex to your thalamus again.
Mm-hmm.
So your brain is a feedback loop.
Right.
Of information.
Wow.
And so...
Do you think it held stake there and then it's like running this, this function across the input and comparing the input to the output?
That it...
Say that again.
Okay.
So like, uh, you said it runs into the thalamus, uh, runs through the brain and back to the thalamus.
Yeah.
So do you think it, it captures state of like, this is the input, uh, runs it through a bunch of different functions and takes the output of those functions and compares it to the original state?
So, okay, so functions are like, you know, uh, if then, you know, like, like, like transfers, like if, like, if or else, right?
But we're talking like...
I guess it's more probabilistic.
I would say it's more, it's more like, um, it's, it's a lot closer to a neural network.
Mm-hmm.
Like, it is kind of like a neural, it's like the natural neural network.
Right.
So it's like, you're running it through a neural network.
So if you look at Google's deep change on my generator, when you have a neural network that's rendering something, and then you send, then you pass through the network again.
Like, so you sit and give it, you train it on photos of Cassie Dawson's names.
And then you, um, and then you give it a photo that's never seen before to be clouds.
And you run it through the neural network and have the lower levels of the neural network render what they do is there.
Right.
And then you run it through that same network again, over and over again.
And eventually it starts to see Cassie Dawson's names in the clouds, just like we've seen things in the clouds, right?
Mm-hmm.
Mm-hmm.
And it's Google's deep, deep dream generator.
Yeah.
If you want to, but, um, and I use that for like the artwork on, um,
San Francisco virtual reality is like artwork.
Like I create the basic, you know, outline in like Photoshop and then I write through the deep dream generator.
Nice.
And it creates all these fancy graphics.
Nice.
And logos for the event.
Um, but, um, yeah.
So the, so the idea there is like if a whole brain is, um, sort of a neural network that like receives the data, renders what's there,
and then pass that to the next, uh, level of neural network.
And it's like every part of the brain is sort of like rendering the frame of reality.
Mm-hmm.
And as it passes through the different parts of the new cortex, you're rendering different things of reality over time.
And so if you take, you take 10 seconds of time and you say, if five, 10 seconds and, uh, like a movie,
you know, like a movie editor, like, oh, do we premiere?
You guys use, you guys use movie editors, right, for your podcasts and stuff.
Yeah.
If you take your consciousness and give that and just put that into a movie editor and say,
this is 10 seconds of consciousness and each frame is made up of neural activity that takes place in this part of the brain,
and then this part of the brain, and then this part of the brain, and then this part of the brain,
and then this part of the brain, you know, along your timeline, right?
And you can say that those are the frames of my consciousness over 10 seconds.
And they're consisting of parts of my neurons that have, in aggregate, detected coincidences patterns
and rendered what they thought was there, you know, based on previous sensory incoming data
and new incoming sensory data.
Right.
And so that's how you potentially render the frames of your mind or your consciousness.
Yeah.
And we potentially have that in that case.
Yeah.
What if you could map, like, all these frames of your mind and then...
So that's why we want to put more sensors in the brain.
Yeah.
And then apply deep neural networks to the task of identifying those brain patterns
that correlate with patterns of the world around us.
So one of the projects that I have and what I've been pitching in my podcast is,
if we take the self-driving car and give it to a person in their backpack, right?
So now you have a self-driving human.
It's not trying to drive.
You're just trying to predict the objects around you.
It's trying to say that this object consists of...
Like, the concept of this object is all these different patterns detected by the artificial
neural network.
And which this matches the brainwave pattern that artificial intelligence has identified
with your mental energy.
And so then if we have that pattern in the computer, then the trick is...
This went into the podcast.
The trick is how do we send that pattern to...
If I can say, I know what this pattern is in terms of brainwaves over time across your brain.
And if we know what this pattern is, and we have it in the computer, how do we send it back into your brain?
And so, you know, I think that the way to do it is you have to see what's there.
Because there's always going to be something there.
Yeah.
And then you have to send the difference between what's there and the pattern that you've captured in the computer.
kind of this.
So if you send the difference, and it causes you to...
It causes the sum of the pattern here to match the pattern of what your brain would have if this light was here.
Then your brain could interpret something in the reality that's not actually there.
Like, okay, what if you're...
When your consciousness is, like, interrupted, like people with PTSD and stuff,
and then all of a sudden, they're, like, reacting to something that is crazy.
You know what I mean?
Okay, people will come back for more.
Like, they relive something.
They hear a lot of noise, and all of a sudden, they're, like...
And they're triggered by whatever it is they're triggered by, and then they're not acting like themselves.
So could we build something that basically is an overlay and changes your consciousness in that moment to know...
I don't know.
Almost, like, to distract you from that.
Yeah, maybe.
Does that make sense?
Yeah, maybe.
Everything that I said could be wrong, but, you know, it's a huge topic.
Yeah.
But maybe I'd rather kind of...
That would be sick.
Yeah.
I mean, that would change people's lives.
It would.
Yeah.
That's so cool.
Wow.
Thanks for coming.
Okay.
That was very thoughtful.
All right.
Should I exit stage work?
Well, actually, it's about time to stop.
I think we're going to wrap up.
Nice.
We had a great hack days.
Yeah.
Now's the time to go around, state your name, where people can find you, social handles,
whatever.
Yeah.
Plug and pitch events, things you're working on, other people's stuff, whatever.
Now's your time to shine, right?
Okay.
So, you want to go first and show them how it's done, and then...
I'll go first.
My name's Catherine.
I am nowhere on the internet, and you can't find me.
But I'll be helping them throw the decentralized conference, and I'll link that down below.
I'll link it again.
But yeah, come to Hack Days on Sundays at 2.30.
Come to the Crypto Builders Meetup.
Is that this Wednesday?
It's this Wednesday.
Oh, we have a Crypto Builders Meetup this Wednesday.
I'll link you guys below as well.
We have our friend Ty coming to speak about how to build your own mining rig, and it's
really cool.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
Yeah.
So, yeah.
That's going to be sick.
I'll link you guys below though.
So, that's how it works.
I'm looking for that because I'm looking to build my own mining rig.
Really?
Are you?
Cool.
Of course, yeah.
Nice.
Of course, you got to make that money.
Sure.
Yeah.
Yeah.
Or maybe if I built it, I can resell it, I don't know.
Yeah.
Totally.
Either way.
There's a market there.
All right.
Now's your time.
to the Neuralist podcast at
http
at colon forward slash forward slash
brma.io
The next article that I write is going to be at
sdgn.io
I also
administrate 29 Facebook groups
on virtual reality, blockchain,
artificial intelligence,
and computational neuroscience, and a whole bunch
of different topics.
Computational biology.
And then you can find the link to those groups
at
http
forward slash forward slash
vrma.
dot
w-o-r-k
And those are my key references. I'm also with
San Francisco Virtual Reality.
The main website is sfbr.net
And we're going to post
our next event for January
probably in the middle of the month
or maybe the 18th, and the website
for finding that information will be
sfbr360.com
Sweet. A lot of stuff.
Good stuff to plug.
Thanks.
Yeah.
Thanks for having me.
Go ahead.
Yeah, yeah, yeah.
I don't really have too much to plug,
at least yet.
Maybe in the future or something.
Cool. If people want to find you
online, connect with you over Twitter
or anything.
I guess, yeah, over Twitter.
I just have...
It's not too active right now.
I'm looking to change that.
Sure.
It's like, it's just
at C-R-A-Q-Q-U-Y
pretty much.
Cool.
All right.
Awesome.
Okay.
And as usual,
I'm Dan Gailey.
You can find me on Twitter
at DPG
and on Facebook,
Dan Gailey.
You can also find me...
I'm the CEO
and co-founder of Synapse.ai
where we're building
decentralized artificial intelligence.
You can check out more about that
in our white paper
and our yellow paper.
Yellow paper is the more technical one.
We are also throwing
the decentralized
artificial intelligence summit.
So if you're working on something
in the space
or you're interested in learning more,
you should hit us up.
We're at decentralized-ai.com.
That's a great domain.
Yeah, right.
It was available,
which is weird.
That's amazing.
Right?
So, yeah,
check us out there
and we have the
Crypto Builders Meetup.
So if you're building anything
in the way of cryptocurrency,
looking to learn more
about programming and solidity,
anything in that field,
it happens every other week
here at Noisebridge.
It's a Crypto Builders Meetup
and it happens at 7 p.m.
So we have,
like what was mentioned earlier,
we have somebody
who's coming out
and talking about mining
and all of their experiences there.
Should be a good talk.
But the audience
is probably the best reason to come.
So we meet other people,
other hackers,
just like you
from all levels.
We also do Hack Days
every Sunday.
So if you're in the Bay Area,
you can come out
and join us
and talk about
all the nerdy stuff
that you love,
that you're working on,
or that other people
are working on.
So you can do that here.
You can find us at
facebook.com
forward slash groups
forward slash Hack Days,
the number four,
all.
You can also find us
on Meetup here locally
at, I don't know,
Crypto Builders Meetup
Hack Days.
Yeah.
I have one more.
So people should join
my Facebook group
Death Star Robot
at deathstarrobot.com.
Okay.
You actually have that domain.
That's a good domain.
How do you spell it?
Death Star.
Oh, Death Star.
Yeah, like Star Wars.
Yes!
I like that.
When I heard
your decent plant AI,
I was like,
oh, I want to share
my cool domain name too.
Yeah, share your
cool domain names.
If you have cool domain names,
share them here.
But, so, as usual,
be excellent to each other
and hack the planet
and we'll see you next week.
Wow, that's sick.
